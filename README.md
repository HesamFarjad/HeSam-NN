# HeSam-Neural Network

The Vision Transformer is an image classification model that utilizes a Transformer architecture across picture patches. The most famous sequence transduction models have built on sophisticated recurrent or convolutional neural networks with an encoder and a decoder. The most refined models additionally use an attention mechanism to connect the encoder and Decoder. Present the Transformer, a new basic network design based simply on attention processes, with no recurrence or convolutions.
In this paper we are going to utilize Vit architecture for Image Captioning task. For doing this action I fine tune the Vit architecture on CIFAR-10 data set with 97.6 percent accuracy.
The process of creating a written description of a picture is known as Image Captioning. The captions will generate using both Natural Language Processing (NLP)  and Computer Vision. In other words, the dataset will be in the for.
 [image â†’ captions]
The dataset consists of input images and their corresponding output captions. Transformers, which are likewise made up of several Self-Attention layers, keep promises for generic learning primitives adaptable to many data types, including the most recent advances in computer vision, achieving State-Of-The-Art (SOTA) efficiency with optimized parameter effectiveness.
